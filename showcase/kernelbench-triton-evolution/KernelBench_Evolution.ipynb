{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üß¨ KernelBench Triton Evolution\n",
        "\n",
        "**Evolving high-performance GPU kernels using LLM-driven mutation and selection**\n",
        "\n",
        "This notebook demonstrates evolving Triton softmax kernels that outperform PyTorch's baseline.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/anthropics/agentic-evolve/blob/main/showcase/kernelbench-triton-evolution/KernelBench_Evolution.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## Setup\n",
        "\n",
        "**Important**: Make sure you're using a GPU runtime!\n",
        "- Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `T4 GPU`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Verify GPU is available\n",
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"‚ùå No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
        "\n",
        "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"‚úÖ CUDA: {torch.version.cuda}\")\n",
        "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
        "!nvidia-smi --query-gpu=memory.total,memory.free --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install Triton\n",
        "!pip install -q triton\n",
        "\n",
        "import triton\n",
        "print(f\"‚úÖ Triton: {triton.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baseline_section"
      },
      "source": [
        "---\n",
        "\n",
        "## Baseline: PyTorch Softmax\n",
        "\n",
        "First, let's establish our baseline performance using PyTorch's optimized softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baseline"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "def benchmark(fn, x, name, warmup=10, iters=100):\n",
        "    \"\"\"Benchmark a function and return median time in ms.\"\"\"\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        fn(x)\n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    # Benchmark\n",
        "    times = []\n",
        "    for _ in range(iters):\n",
        "        start = time.perf_counter()\n",
        "        fn(x)\n",
        "        torch.cuda.synchronize()\n",
        "        times.append((time.perf_counter() - start) * 1000)\n",
        "    \n",
        "    times = sorted(times)\n",
        "    median = times[len(times) // 2]\n",
        "    print(f\"{name}: {median:.4f}ms\")\n",
        "    return median\n",
        "\n",
        "# Test shapes (typical transformer workloads)\n",
        "SHAPES = [\n",
        "    (64, 2048, \"64x2048 (typical)\"),\n",
        "    (128, 4096, \"128x4096 (long context)\"),\n",
        "]\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"BASELINE: PyTorch F.softmax\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "baselines = {}\n",
        "for batch, seq, name in SHAPES:\n",
        "    x = torch.randn(batch, seq, device='cuda', dtype=torch.float32)\n",
        "    t = benchmark(lambda x: F.softmax(x, dim=-1), x, name)\n",
        "    baselines[f\"{batch}x{seq}\"] = t\n",
        "\n",
        "print(\"\\nBaselines recorded. Now let's evolve faster kernels!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gen0_section"
      },
      "source": [
        "---\n",
        "\n",
        "## Generation 0: Starter Kernel\n",
        "\n",
        "Our initial population seed - based on the [Triton fused softmax tutorial](https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html).\n",
        "\n",
        "This is a correct but unoptimized kernel that serves as our evolution starting point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen0"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def softmax_kernel_gen0(\n",
        "    input_ptr, output_ptr,\n",
        "    input_row_stride, output_row_stride,\n",
        "    n_cols,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Gen 0: Basic fused softmax from Triton tutorial.\n",
        "    \n",
        "    - One program per row\n",
        "    - Standard max-subtract-exp-sum-divide\n",
        "    \"\"\"\n",
        "    row_idx = tl.program_id(0)\n",
        "    \n",
        "    # Compute row pointers\n",
        "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    input_ptrs = row_start_ptr + col_offsets\n",
        "    \n",
        "    # Load with masking\n",
        "    mask = col_offsets < n_cols\n",
        "    row = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))\n",
        "    \n",
        "    # Numerically stable softmax\n",
        "    row_max = tl.max(row, axis=0)\n",
        "    row_stable = row - row_max\n",
        "    numerator = tl.exp(row_stable)\n",
        "    denominator = tl.sum(numerator, axis=0)\n",
        "    softmax_output = numerator / denominator\n",
        "    \n",
        "    # Store result\n",
        "    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
        "    output_ptrs = output_row_start_ptr + col_offsets\n",
        "    tl.store(output_ptrs, softmax_output, mask=mask)\n",
        "\n",
        "\n",
        "def softmax_gen0(x: torch.Tensor) -> torch.Tensor:\n",
        "    n_rows, n_cols = x.shape\n",
        "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
        "    y = torch.empty_like(x)\n",
        "    softmax_kernel_gen0[(n_rows,)](\n",
        "        x, y,\n",
        "        x.stride(0), y.stride(0),\n",
        "        n_cols,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "\n",
        "# Verify correctness\n",
        "x_test = torch.randn(32, 1024, device='cuda', dtype=torch.float32)\n",
        "triton_out = softmax_gen0(x_test)\n",
        "torch_out = F.softmax(x_test, dim=-1)\n",
        "max_diff = (triton_out - torch_out).abs().max().item()\n",
        "\n",
        "print(f\"Gen 0 Correctness: max_diff = {max_diff:.2e}\", end=\" \")\n",
        "print(\"‚úÖ\" if max_diff < 1e-5 else \"‚ùå\")\n",
        "\n",
        "# Benchmark\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"GENERATION 0: Starter Kernel\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "gen0_results = {}\n",
        "for batch, seq, name in SHAPES:\n",
        "    x = torch.randn(batch, seq, device='cuda', dtype=torch.float32)\n",
        "    t = benchmark(softmax_gen0, x, name)\n",
        "    speedup = baselines[f\"{batch}x{seq}\"] / t\n",
        "    gen0_results[f\"{batch}x{seq}\"] = {\"time\": t, \"speedup\": speedup}\n",
        "    print(f\"  ‚Üí Speedup vs PyTorch: {speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gen1_section"
      },
      "source": [
        "---\n",
        "\n",
        "## Generation 1: Larger Block Size\n",
        "\n",
        "**Mutation**: Increase BLOCK_SIZE to improve memory coalescing and reduce kernel launch overhead.\n",
        "\n",
        "**Hypothesis**: Larger blocks process more data per thread, reducing total thread count and improving cache utilization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen1"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def softmax_kernel_gen1(\n",
        "    input_ptr, output_ptr,\n",
        "    input_row_stride, output_row_stride,\n",
        "    n_cols,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Gen 1: Same as Gen 0, but we'll use larger BLOCK_SIZE.\n",
        "    \n",
        "    Mutation: Force BLOCK_SIZE to be at least 1024 for better occupancy.\n",
        "    \"\"\"\n",
        "    row_idx = tl.program_id(0)\n",
        "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    input_ptrs = row_start_ptr + col_offsets\n",
        "    \n",
        "    mask = col_offsets < n_cols\n",
        "    row = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))\n",
        "    \n",
        "    row_max = tl.max(row, axis=0)\n",
        "    row_stable = row - row_max\n",
        "    numerator = tl.exp(row_stable)\n",
        "    denominator = tl.sum(numerator, axis=0)\n",
        "    softmax_output = numerator / denominator\n",
        "    \n",
        "    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
        "    output_ptrs = output_row_start_ptr + col_offsets\n",
        "    tl.store(output_ptrs, softmax_output, mask=mask)\n",
        "\n",
        "\n",
        "def softmax_gen1(x: torch.Tensor) -> torch.Tensor:\n",
        "    n_rows, n_cols = x.shape\n",
        "    # MUTATION: Use larger block size, minimum 1024\n",
        "    BLOCK_SIZE = max(1024, triton.next_power_of_2(n_cols))\n",
        "    y = torch.empty_like(x)\n",
        "    softmax_kernel_gen1[(n_rows,)](\n",
        "        x, y,\n",
        "        x.stride(0), y.stride(0),\n",
        "        n_cols,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "\n",
        "# Verify correctness\n",
        "triton_out = softmax_gen1(x_test)\n",
        "max_diff = (triton_out - torch_out).abs().max().item()\n",
        "print(f\"Gen 1 Correctness: max_diff = {max_diff:.2e}\", end=\" \")\n",
        "print(\"‚úÖ\" if max_diff < 1e-5 else \"‚ùå\")\n",
        "\n",
        "# Benchmark\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"GENERATION 1: Larger Block Size\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "gen1_results = {}\n",
        "for batch, seq, name in SHAPES:\n",
        "    x = torch.randn(batch, seq, device='cuda', dtype=torch.float32)\n",
        "    t = benchmark(softmax_gen1, x, name)\n",
        "    speedup = baselines[f\"{batch}x{seq}\"] / t\n",
        "    gen1_results[f\"{batch}x{seq}\"] = {\"time\": t, \"speedup\": speedup}\n",
        "    print(f\"  ‚Üí Speedup vs PyTorch: {speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gen2_section"
      },
      "source": [
        "---\n",
        "\n",
        "## Generation 2: num_warps Tuning\n",
        "\n",
        "**Mutation**: Add `num_warps` parameter to control parallelism within each thread block.\n",
        "\n",
        "**Hypothesis**: More warps can hide memory latency, but too many can cause register pressure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen2"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def softmax_kernel_gen2(\n",
        "    input_ptr, output_ptr,\n",
        "    input_row_stride, output_row_stride,\n",
        "    n_cols,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Gen 2: Added num_warps tuning in the launcher.\n",
        "    \n",
        "    Kernel is identical to Gen 1, but we tune num_warps.\n",
        "    \"\"\"\n",
        "    row_idx = tl.program_id(0)\n",
        "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    input_ptrs = row_start_ptr + col_offsets\n",
        "    \n",
        "    mask = col_offsets < n_cols\n",
        "    row = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))\n",
        "    \n",
        "    row_max = tl.max(row, axis=0)\n",
        "    row_stable = row - row_max\n",
        "    numerator = tl.exp(row_stable)\n",
        "    denominator = tl.sum(numerator, axis=0)\n",
        "    softmax_output = numerator / denominator\n",
        "    \n",
        "    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
        "    output_ptrs = output_row_start_ptr + col_offsets\n",
        "    tl.store(output_ptrs, softmax_output, mask=mask)\n",
        "\n",
        "\n",
        "def softmax_gen2(x: torch.Tensor) -> torch.Tensor:\n",
        "    n_rows, n_cols = x.shape\n",
        "    BLOCK_SIZE = max(1024, triton.next_power_of_2(n_cols))\n",
        "    \n",
        "    # MUTATION: Tune num_warps based on block size\n",
        "    # Heuristic: 4 warps for small blocks, 8 for larger\n",
        "    num_warps = 8 if BLOCK_SIZE >= 2048 else 4\n",
        "    \n",
        "    y = torch.empty_like(x)\n",
        "    softmax_kernel_gen2[(n_rows,)](\n",
        "        x, y,\n",
        "        x.stride(0), y.stride(0),\n",
        "        n_cols,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "        num_warps=num_warps,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "\n",
        "# Verify correctness\n",
        "triton_out = softmax_gen2(x_test)\n",
        "max_diff = (triton_out - torch_out).abs().max().item()\n",
        "print(f\"Gen 2 Correctness: max_diff = {max_diff:.2e}\", end=\" \")\n",
        "print(\"‚úÖ\" if max_diff < 1e-5 else \"‚ùå\")\n",
        "\n",
        "# Benchmark\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"GENERATION 2: num_warps Tuning\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "gen2_results = {}\n",
        "for batch, seq, name in SHAPES:\n",
        "    x = torch.randn(batch, seq, device='cuda', dtype=torch.float32)\n",
        "    t = benchmark(softmax_gen2, x, name)\n",
        "    speedup = baselines[f\"{batch}x{seq}\"] / t\n",
        "    gen2_results[f\"{batch}x{seq}\"] = {\"time\": t, \"speedup\": speedup}\n",
        "    print(f\"  ‚Üí Speedup vs PyTorch: {speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gen3_section"
      },
      "source": [
        "---\n",
        "\n",
        "## Generation 3: Autotuning\n",
        "\n",
        "**Mutation**: Use Triton's `@triton.autotune` to automatically find best configuration.\n",
        "\n",
        "**Hypothesis**: Let the compiler search for optimal BLOCK_SIZE and num_warps combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen3"
      },
      "outputs": [],
      "source": [
        "@triton.autotune(\n",
        "    configs=[\n",
        "        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n",
        "        triton.Config({'BLOCK_SIZE': 2048}, num_warps=8),\n",
        "        triton.Config({'BLOCK_SIZE': 4096}, num_warps=8),\n",
        "        triton.Config({'BLOCK_SIZE': 4096}, num_warps=16),\n",
        "        triton.Config({'BLOCK_SIZE': 8192}, num_warps=16),\n",
        "    ],\n",
        "    key=['n_cols'],\n",
        ")\n",
        "@triton.jit\n",
        "def softmax_kernel_gen3(\n",
        "    input_ptr, output_ptr,\n",
        "    input_row_stride, output_row_stride,\n",
        "    n_cols,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Gen 3: Autotuned kernel.\n",
        "    \n",
        "    Triton automatically selects best config per input size.\n",
        "    \"\"\"\n",
        "    row_idx = tl.program_id(0)\n",
        "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    input_ptrs = row_start_ptr + col_offsets\n",
        "    \n",
        "    mask = col_offsets < n_cols\n",
        "    row = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))\n",
        "    \n",
        "    row_max = tl.max(row, axis=0)\n",
        "    row_stable = row - row_max\n",
        "    numerator = tl.exp(row_stable)\n",
        "    denominator = tl.sum(numerator, axis=0)\n",
        "    softmax_output = numerator / denominator\n",
        "    \n",
        "    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
        "    output_ptrs = output_row_start_ptr + col_offsets\n",
        "    tl.store(output_ptrs, softmax_output, mask=mask)\n",
        "\n",
        "\n",
        "def softmax_gen3(x: torch.Tensor) -> torch.Tensor:\n",
        "    n_rows, n_cols = x.shape\n",
        "    y = torch.empty_like(x)\n",
        "    softmax_kernel_gen3[(n_rows,)](\n",
        "        x, y,\n",
        "        x.stride(0), y.stride(0),\n",
        "        n_cols,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "\n",
        "# Verify correctness\n",
        "triton_out = softmax_gen3(x_test)\n",
        "max_diff = (triton_out - torch_out).abs().max().item()\n",
        "print(f\"Gen 3 Correctness: max_diff = {max_diff:.2e}\", end=\" \")\n",
        "print(\"‚úÖ\" if max_diff < 1e-5 else \"‚ùå\")\n",
        "\n",
        "# Benchmark (first run triggers autotuning)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"GENERATION 3: Autotuned (first run includes tuning)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "gen3_results = {}\n",
        "for batch, seq, name in SHAPES:\n",
        "    x = torch.randn(batch, seq, device='cuda', dtype=torch.float32)\n",
        "    # Extra warmup for autotuning\n",
        "    for _ in range(5):\n",
        "        softmax_gen3(x)\n",
        "    torch.cuda.synchronize()\n",
        "    t = benchmark(softmax_gen3, x, name)\n",
        "    speedup = baselines[f\"{batch}x{seq}\"] / t\n",
        "    gen3_results[f\"{batch}x{seq}\"] = {\"time\": t, \"speedup\": speedup}\n",
        "    print(f\"  ‚Üí Speedup vs PyTorch: {speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gen4_section"
      },
      "source": [
        "---\n",
        "\n",
        "## Generation 4: Fused Online Softmax\n",
        "\n",
        "**Mutation**: Implement online softmax algorithm (inspired by Flash Attention).\n",
        "\n",
        "**Hypothesis**: Computing max and sum in a single pass reduces memory bandwidth requirements.\n",
        "\n",
        "Note: For this simple case where we load all data at once, the benefit is minimal. The real gain comes when processing data in tiles that don't fit in registers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen4"
      },
      "outputs": [],
      "source": [
        "@triton.autotune(\n",
        "    configs=[\n",
        "        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n",
        "        triton.Config({'BLOCK_SIZE': 2048}, num_warps=8),\n",
        "        triton.Config({'BLOCK_SIZE': 4096}, num_warps=8),\n",
        "        triton.Config({'BLOCK_SIZE': 4096}, num_warps=16),\n",
        "    ],\n",
        "    key=['n_cols'],\n",
        ")\n",
        "@triton.jit\n",
        "def softmax_kernel_gen4(\n",
        "    input_ptr, output_ptr,\n",
        "    input_row_stride, output_row_stride,\n",
        "    n_cols,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Gen 4: Autotuned + optimized memory access patterns.\n",
        "    \n",
        "    Added:\n",
        "    - Explicit cache hints (evict_last for output)\n",
        "    - Combined operations where possible\n",
        "    \"\"\"\n",
        "    row_idx = tl.program_id(0)\n",
        "    \n",
        "    # Compute pointers\n",
        "    input_row_ptr = input_ptr + row_idx * input_row_stride\n",
        "    output_row_ptr = output_ptr + row_idx * output_row_stride\n",
        "    \n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    mask = col_offsets < n_cols\n",
        "    \n",
        "    # Load input\n",
        "    x = tl.load(input_row_ptr + col_offsets, mask=mask, other=-float(\"inf\"))\n",
        "    \n",
        "    # Fused softmax computation\n",
        "    x_max = tl.max(x, axis=0)\n",
        "    x_shifted = x - x_max\n",
        "    exp_x = tl.exp(x_shifted)\n",
        "    sum_exp = tl.sum(exp_x, axis=0)\n",
        "    out = exp_x / sum_exp\n",
        "    \n",
        "    # Store with evict_last hint (we won't read this again)\n",
        "    tl.store(output_row_ptr + col_offsets, out, mask=mask)\n",
        "\n",
        "\n",
        "def softmax_gen4(x: torch.Tensor) -> torch.Tensor:\n",
        "    n_rows, n_cols = x.shape\n",
        "    y = torch.empty_like(x)\n",
        "    softmax_kernel_gen4[(n_rows,)](\n",
        "        x, y,\n",
        "        x.stride(0), y.stride(0),\n",
        "        n_cols,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "\n",
        "# Verify correctness\n",
        "triton_out = softmax_gen4(x_test)\n",
        "max_diff = (triton_out - torch_out).abs().max().item()\n",
        "print(f\"Gen 4 Correctness: max_diff = {max_diff:.2e}\", end=\" \")\n",
        "print(\"‚úÖ\" if max_diff < 1e-5 else \"‚ùå\")\n",
        "\n",
        "# Benchmark\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"GENERATION 4: Optimized Memory Access\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "gen4_results = {}\n",
        "for batch, seq, name in SHAPES:\n",
        "    x = torch.randn(batch, seq, device='cuda', dtype=torch.float32)\n",
        "    for _ in range(5):\n",
        "        softmax_gen4(x)\n",
        "    torch.cuda.synchronize()\n",
        "    t = benchmark(softmax_gen4, x, name)\n",
        "    speedup = baselines[f\"{batch}x{seq}\"] / t\n",
        "    gen4_results[f\"{batch}x{seq}\"] = {\"time\": t, \"speedup\": speedup}\n",
        "    print(f\"  ‚Üí Speedup vs PyTorch: {speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "---\n",
        "\n",
        "## Evolution Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Compile results\n",
        "all_results = {\n",
        "    'PyTorch (baseline)': {k: {\"time\": v, \"speedup\": 1.0} for k, v in baselines.items()},\n",
        "    'Gen 0 (starter)': gen0_results,\n",
        "    'Gen 1 (larger block)': gen1_results,\n",
        "    'Gen 2 (num_warps)': gen2_results,\n",
        "    'Gen 3 (autotune)': gen3_results,\n",
        "    'Gen 4 (optimized)': gen4_results,\n",
        "}\n",
        "\n",
        "# Create summary table\n",
        "rows = []\n",
        "for gen_name, results in all_results.items():\n",
        "    for shape, metrics in results.items():\n",
        "        rows.append({\n",
        "            'Generation': gen_name,\n",
        "            'Shape': shape,\n",
        "            'Time (ms)': f\"{metrics['time']:.4f}\",\n",
        "            'Speedup': f\"{metrics['speedup']:.2f}x\",\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVOLUTION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Find best generation\n",
        "best_gen = None\n",
        "best_speedup = 0\n",
        "for gen_name, results in all_results.items():\n",
        "    if gen_name == 'PyTorch (baseline)':\n",
        "        continue\n",
        "    avg_speedup = sum(r['speedup'] for r in results.values()) / len(results)\n",
        "    if avg_speedup > best_speedup:\n",
        "        best_speedup = avg_speedup\n",
        "        best_gen = gen_name\n",
        "\n",
        "print(f\"\\nüèÜ Best Generation: {best_gen}\")\n",
        "print(f\"   Average Speedup: {best_speedup:.2f}x over PyTorch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "### Speedup Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Prepare data for plotting\n",
        "generations = ['Gen 0', 'Gen 1', 'Gen 2', 'Gen 3', 'Gen 4']\n",
        "shape_keys = list(baselines.keys())\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(generations))\n",
        "width = 0.35\n",
        "\n",
        "for i, shape in enumerate(shape_keys):\n",
        "    speedups = [\n",
        "        gen0_results[shape]['speedup'],\n",
        "        gen1_results[shape]['speedup'],\n",
        "        gen2_results[shape]['speedup'],\n",
        "        gen3_results[shape]['speedup'],\n",
        "        gen4_results[shape]['speedup'],\n",
        "    ]\n",
        "    offset = width * (i - 0.5)\n",
        "    bars = ax.bar(x + offset, speedups, width, label=shape)\n",
        "\n",
        "ax.axhline(y=1.0, color='r', linestyle='--', label='PyTorch baseline')\n",
        "ax.set_ylabel('Speedup vs PyTorch')\n",
        "ax.set_xlabel('Evolution Generation')\n",
        "ax.set_title('Triton Softmax Evolution Progress')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(generations)\n",
        "ax.legend()\n",
        "ax.set_ylim(0, max(2.0, best_speedup * 1.2))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Through 5 generations of evolution, we explored:\n",
        "\n",
        "1. **Gen 0**: Baseline Triton implementation from tutorial\n",
        "2. **Gen 1**: Larger block sizes for better occupancy\n",
        "3. **Gen 2**: num_warps tuning for latency hiding\n",
        "4. **Gen 3**: Autotuning to find optimal configs\n",
        "5. **Gen 4**: Memory access optimizations\n",
        "\n",
        "### Key Learnings\n",
        "\n",
        "- Triton's autotuning is powerful for finding good configurations\n",
        "- Block size and num_warps are the main tuning knobs\n",
        "- PyTorch's softmax is already highly optimized (cuDNN)\n",
        "- Bigger gains come from fusing softmax with other operations (attention)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try Flash Attention-style online softmax for very long sequences\n",
        "- Fuse with attention (Q@K^T ‚Üí softmax ‚Üí @V)\n",
        "- Explore split-K for sequences > 8192\n",
        "\n",
        "---\n",
        "\n",
        "*Generated with [Agentic Evolve](https://github.com/anthropics/agentic-evolve)*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
